\chapter{Układy programowalne}
Kiedy w 1985 roku, dzięki szybko rozwijającej się litografii, wprowadzono pierwszy komercyjnie dostępny układ typu FPGA~(ang. \textit{Field Programmable Gate Array}) - układ XC2064 - bardzo szybko doceniono jego zalety wszędzie tam, gdzie wymagana jest łatwa zmiana funkcjonalności systemu przetwarzania danych. Głównym motorem rozwoju, szczególnie na początku, było powstanie i gwałtowna ekspansja Internetu i konieczność łatwego prototypowania oraz wdrażania nowych rozwiązań zwłaszcza jeśli chodzi o tworzenie przełączników i ruterów~\cite{Designing_with_Xilinx}.

Tak duże zainteresowanie tymi układami bierze się z faktu, iż są one w łatwy sposób rekonfigurowalne, a przez to mogą realizować dowolne funkcje logiczne. W przeciwieństwie do ogólnodostępnych układów typu CPU~(ang. \textit{Central Processing Unit}), których działanie opiera się na sekwencyjnym przetwarzaniu listy rozkazów w obrębie przewidzianej przez producenta \textit{architektury instrukcji}~(ang.~\textit{Instruction Set Architecture}, ISA)~\cite{INTEL_ISA} układy FPGA są konfigurowalne na poziomie sprzętowym a nie programowane. Zakupiony od producenta procesor po podaniu mu danych oraz listy instrukcji~(programu) będzie je przetwarzał i na wyjściu otrzyma się oczekiwaną wartość. Układ FPGA nie zrealizuje żadnej operacji mimo podania konkretnych danych wejściowych, jeśli nie zostanie skonfigurowany\footnote{Rozróżnienie pomiędzy konfiguracją a programowaniem powinno wybrzmieć dostatecznie mocno. Na dalszych kartach tej pracy terminy te będą używane zamiennie w kontekście układów FPGA, co jest podyktowane technologią użytą do wykonania projektu magisterskiego.}.

Na konfigurowalność układów FPGA składają się wspólnie dwa czynniki - obecność \textit{bloków logicznych}~(ang.~\textit{logic blocks}) oraz elastycznej~(tzn. konfigurowalnej) sieci połączeń między nimi~(ang. \textit{routing}) zdolnej teoretycznie wytworzyć połączenie pomiędzy dowolnymi blokami.
\addimage{chapters/ch2/img/fpgaBigPicture.png}{scale=0.4}{Schemat ideowy budowy układu FPGA. Bloki logiczne (LB) połączone są między sobą oraz z blokami wejścia-wyjścia~(IOB) za pomocą gęstej sieci konfigurowalnych połączeń. Bloki wejścia-wyjścia służą do ustanowienia komunikacji z urządzeniami peryferyjnymi takimi jak moduł pamięci RAM czy klawiatura}{Schemat ideowy budowy układu FPGA}{ch2:img:fpgaBigPicture}
Każdy blok logiczny składa się minimalnie z trzech elementów:
\begin{itemize}
\item n-wejściowej konfigurowalnej \textit{tablicy przeglądowej}~(ang. \textit{LookUp Table}, LUT), której zadaniem jest realizacja n-parametrowej funkcji logicznej np. $(\bar{A}\vee B) \wedge C$ jest 3-parametrową funkcją logiczną,
\item \textit{przerzutnika}~(ang. \textit{flip-flop}, FF) działającego jako pamięć,
\item \textit{multipleksera}~(ang. \textit{multiplexer}, MUX/MX), który dokonuje wyboru źródła sygnału, który ma zostać przekazany na wyjście bloku logicznego.
\end{itemize}
\addimage{chapters/ch2/img/fpgaLB}{scale=0.35}{Budowa podstawowego bloku logicznego w układzie FPGA. 4-wejściowy LUT realizuje ustaloną funkcję logiczną, której wynik, zależny od kombinacji sygnałów (I[0]:I[3]), przekazywany jest do przerzutnika (FF) w celu zapamiętania. Przerzutnik działa synchronicznie z doprowadzonym sygnałem zegarowym~(CLK) a jego zawartość może być zresetowana~(RST). O tym, jaka wartość~(O) zostanie podana na wyjściu bloku logicznego decyduje parametr~(S) sterujący wyborem multipleksera~(MX) pomiędzy nowym a zapamiętanym uprzednio wynikiem działania.}{Budowa podstawowego bloku logicznego w układzie FGPA}{ch2:img:fpgaLB}
Pojedynczy blok logiczny sam w sobie jest dość prymitywnym elementem i samodzielnie nie jest w stanie realizować funkcji, której liczba parametrów wejściowych przekracza ilość wejść do LUT. Jednak dzięki występującej sieci połączeń\footnote{W literaturze często wspomina się o blokach logicznych zanurzonych w morzu połączeń.} wyjście jednego bloku logicznego może stanowić wejście drugiego. W ten sposób tworzone są znacznie bardziej rozbudowane funkcje logiczne. Wynika też z tego, że im więcej i im bardziej rozbudowane są bloki logiczne tym możliwości w zakresie komponowania funkcjonalności są szersze~(wtedy jednak pojawiają się dodatkowe wyzwania konstruktorskie związane z optymalnym projektem sieci połączeń~\cite{FPGA_ARCHITECTURE}).
\addimage{chapters/ch2/img/fpgaSizeCost.png}{scale=0.4}{Rozwój układów FPGA od momentu ich konstrukcji do dziś. Wartości podane na wykresie są liczone względem pierwotnego układu XC2064, który posiadał 64 programowalne bloki logiczne. Kwadratami oznaczono względną pojemność układu rozumianą poprzez ilość elementów logicznych, zamalowane koła osiągalne częstotliwości, ciągła gruba linia to cena, a linia z krzyżykami to pobierana moc. Widoczny jest znaczny postęp, który dokonał się na przestrzeni ostatnich lat~\cite{Designing_with_Xilinx}}{Rozwój układów FPGA od momentu ich konstrukcji do dziś}{ch2:img:fpgaSizeCost}

Lata rozwoju układów FPGA połączone z analizą często wykorzystywanych algorytmów doprowadziły do implementacji dodatkowych elementów wchodzących w skład bloków logicznych. Są to jednostki bardziej wyspecjalizowane niż LUT, których użycie sprawia, że dany typ zadania może być wykonany znacznie szybciej i/lub jest bardziej ekonomiczne pod względem wykorzystania powierzchni układu. Tak powstały m. in:
\begin{itemize}
\item pamięci o niewielkiej pojemności LUTRAM,
\item pamięci blokowe~(ang. \textit{block RAM, BRAM}), zdolne do przechowywania porcji danych rzędu 18~kb i wykonania do dwóch operacji dostępu w jednym cyklu zegara,
\item moduły DSP dedykowane szybkiemu przetwarzaniu operacji dodawania i mnożenia.
\end{itemize}
Znajomość liczby, typu elementów składających się na układ FPGA oraz wymagań stawianych na etapie projektowania algorytmów pozwala dobrać odpowiedni układ do danych zastosowań, a przez to zoptymalizować koszty, które generowane byłyby przez elementy niewykorzystane. Producenci, chcąc sprostać oczekiwaniom rynkowym dostarczają całe gamy produktów, których wielkość mierzona za pomocą ilości bloków logicznych może zaczynać się na ok. 1000 bloków a kończyć na paru milionach~\cite{XILINX_PRODUCT_TABLE}.

Elastyczność w zakresie budowania funkcji logicznych posiada jednakże pewną niedogodność związaną z maksymalnymi osiągalnymi częstotliwościami pracy układów. W części przypadków~(w tym omawianego systemu śledzenia promieni) samo przetworzenie sygnałów w bloku logicznym może trwać zaledwie 20-30\% budżetu czasowego wynikającego z okresu zadanego zegara. Pozostały czas jest czasem potrzebnym na propagację wyniku do kolejnego bloku logicznego poprzez dostępną w układzie konfigurowalną sieć połączeń. W efekcie uzyskiwane maksymalne częstotliwości zegara dla specyficznych zastosowań przetwarzania sygnałów~(ang. \textit{Digital Signal Processing, DSP}) mogą przekraczać 500~MHz\footnote{Tak wysokie częstotliwości, jak na układy FPGA są możliwe do uzyskania tylko dzięki obecności dedykowanych bloków DSP w strukturze układu FPGA.}, a w typowych zastosowaniach są to wartości pomiędzy 200 a 400 MHz~(wartości te zależą oczywiście od technologii wykonania układu FPGA, akcelerowanego algorytmu i jakości kodu). W porównaniu do obecnych dzisiaj procesorów CPU działających coraz częściej z częstotliwościami sięgającymi 4 GHz są to wartości 10- 20-krotnie niższe. Wydawać mogłoby się zatem~(tylko na podstawie porównania osiąganych częstotliwości), że nie można oczekiwać zbliżonej i wyższej wydajności od układu FPGA realizującego identyczny algorytm co odpowiadający mu procesor CPU. Nie musi być to prawdą. Zależnie od typu rozwiązywanego problemu jak i umiejętności projektanta, algorytmy mogą być wykonywane o całe rzędy wielkości szybciej niż ich odpowiedniki opisane listą rozkazów CPU przy niższym zegarze oraz niższej konsumpcji energii elektrycznej. Kluczowa jest tu świadomość, iż projektant ma bezpośredni wpływ na to, jak układ zostanie skonfigurowany. Ta dowolność nierozerwalnie wiąże się z dużą odpowiedzialnością oraz stopniem doświadczenia wymaganego od projektanta.

\section{Konfiguracja i języki opisu sprzętu}
Pierwsze wytworzone \textit{układy scalone}~(ang. \textit{integrated circuit}, IC) posiadały małą złożoność, liczoną w setkach bramek logicznych, przez to charakteryzowały się niewielkim stopniem komplikacji (w rozumieniu realizowanej funkcjonalności). W tamtych czasach odpowiedzialność za projekt i testy takiego układu leżały w gestii doświadczonego projektanta. Musiał on nie tylko w sposób ręczny wykonać projekt realizujący zadaną funkcjonalność, ale również zadbać o to, by sygnały propagujące się w układzie dochodziły do kolejnych etapów przetwarzania, wtedy gdy będą potrzebne\footnote{Prędkość propagacji sygnałów jest skończona i wynika między innymi z wykorzystywanej technologii.}. Wraz z postępem technologicznym ilość bramek logicznych, która mogła znaleźć się w pojedynczym układzie rosła, w wyniku czego pojawiła się potrzeba stworzenia narzędzi, które mogłyby wspomagać pracę na kolejnych etapach tworzenia układu elektronicznego tj.:
\begin{itemize}
\item tworzenia funkcjonalności,
\item testów, 
\item syntezy,
\item implementacji. 
\end{itemize}
W tym czasie języki programowania takie jak Pascal i C były powszechnie używane do tworzenia programów wykonywanych sekwencyjnie przez procesory zdolne do realizowania określonego zbioru instrukcji. Języki te jednakże nie mogły zostać zaadaptowane do celów opisu funkcjonalności połączonych ze sobą elementów elektronicznych (pojedynczych bramek, bloków DSP czy LUT) właśnie przez ich sekwencyjną naturę. Zintegrowane układy elektroniczne w swoim założeniu przetwarzają jednocześnie wiele sygnałów dla optymalnej efektywności. Dla przykładu, jeżeli rozwiązywany problem wymaga wykonania $n$ niezależnych od siebie operacji dodawania, najefektywniej jest zaprojektować układ tak, by na tym etapie przetwarzania znajdowało się $n$ niezależnych od siebie sumatorów. Potrzeba kreowania tego typu zachowań w układach elektronicznych doprowadziła do powstania \textit{języków opisu sprzętu}~(ang. \textit{Hardware Description Languages}, HDL). Pozwoliły one nie tylko na modelowanie funkcjonalności układów elektronicznych, ale również na przeprowadzanie wnikliwych testów poprawności wykonania zgodnie z założeniami projektowymi dzięki środowiskom symulacyjnym. Języki te, najpopularniejsze wśród nich to Verilog oraz VHDL, zaoferowały możliwość opisu układów cyfrowych na poziomie przepływu danych pomiędzy kolejnymi rejestrami~(ang. \textit{register transfer level}, RTL). Odrębne narzędzia zaś na podstawie tego opisu~(RTL) mogą dokonać ekstrakcji wymaganych elementów logicznych i połączeń między nimi~\cite{VERILOG_BIBLE}. W zależności od tego, jakiego typu układ jest projektowany: czy jest to \textit{wyspecjalizowany układ scalony}~(ang. \textit{application-specific integrated circuit}, ASIC) czy układ FPGA, zespół wyekstrahowanych elementów logicznych i połączeń między nimi tzw. \textit{netlista}~(ang. \textit{netlist}) na etapie syntezy będzie inny, ale funkcjonalność zostanie zachowana. Właśnie ta cecha jest często eksploatowana przy tworzeniu układów typu ASIC, gdzie etapem pośrednim jest implementacja funkcjonalności w układzie FPGA, co pozwala przetestować w realnych zastosowaniach nowo tworzony układ, zanim ten zostanie wysłany do~(kosztownej) produkcji w finalne postaci{\color{red}CYTOWANIE}. 

Implementacja w układzie FPGA jest zautomatyzowanym procesem zaczynającym się od optymalizacji wygenerowanej netlisty, po to aby ta zajmowała jak najmniej bloków logicznych. Ma to na celu zmniejszenie opóźnień związanych z propagacją danych, dzięki czemu możliwe jest użycie wyższych częstotliwości zegarów. Następnie bloki te łączone są ze sobą w klastry w fizycznym układzie w taki sposób, by uzyskać ich optymalny rozkład w przestrzeni układu~(unikając tzw. \textit{stłoczenia}~ ang. \textit{congestion}, czyli lokalnego przeciążenia sieci połączeń) z zachowaniem jak najkrótszych odległości między nimi. 

Naturalnym problemem, który tutaj występuje jest pytanie o to czy rezultaty uzyskane na drodze syntezy i implementacji są najlepsze z możliwych dla danego układu FPGA i opisu za pomocą RTL. W istocie dla dzisiejszych układów, których ilość bloków logicznych przekracza milion, a ilość dostępnych połączeń w sieci jest dziesięciokrotnie wyższa rozpatrzenie wszelkich możliwych kombinacji jest niemożliwe~\cite{FPGA_SD}. W związku z tym narzędzia muszą posługiwać się przybliżonymi algorytmami heurystycznymi sterowanymi za pomocą dziesiątek parametrów a i tak czas oczekiwania na finalny plik konfiguracyjny tzw. \textit{bitstream} może przekroczyć 24 godziny~(w zależności od stopnia skomplikowania RTL). Co więcej nie ma gwarancji, że narzędzie znajdzie rozwiązanie, które spełnia wymagania czasowe dotyczące transferu sygnałów pomiędzy kolejnymi blokami - bez tego układ na pewno nie będzie działał poprawnie\footnote{Narzędzia dokonujące wyliczeń opóźnień między blokami z natury są konserwatywne, czyli ich estymacje wynikają z symulacji najgorszego możliwego przypadku. Znane są jednak przypadki, gdy narzędzia te raportowały pozytywne przejście testów opóźnień, jednak pracujące urządzenie po pewnym czasie zaczynało pracować w sposób nieprzewidywalny. Błędy te najprawdopodobniej związane są ze zwiększonym szumem termicznym występującym w pracującym układzie. }. Na dodatek wpływ dwóch dowolnych parametrów sterujących syntezą i implementacją nie musi być od siebie niezależny. Dlatego producenci tych narzędzi dostarczają przeważnie zestawy predefiniowanych ustawień, które według ich zapewnień powinny działać najbardziej optymalnie. Ustawienia te można podzielić na dwie główne i z założenia wykluczające się grupy: optymalizujące projekt pod względem osiąganych częstotliwości zegara oraz ilości użytych bloków logicznych. Jednak każdy projekt, a zwłaszcza te o dużym stopniu utylizacji zasobów układu, należy traktować indywidualnie. Samodzielne poszukiwanie optymalnych parametrów przy ustalonym RTL, gdy predefiniowane ustawienia nie dają oczekiwanych rezultatów wymaga nie tylko doświadczenia, ale również szczęścia. InTime, czyli komercyjne narzędzie dostępne na rynku i rozwijane od kilku lat, pozwala poprzez użycie technik uczenia maszynowego rozwiązać większość tych problemów. Jest ono tak zaprojektowane, iż jest w stanie dokonać predykcji optymalnych parametrów syntezy i implementacji każdego projektu dla narzędzi dostarczanych przez najważniejszych producentów układów FPGA~\cite{InTime1}\cite{InTime2}\cite{InTime3}. 

Z drugiej zaś strony, istnieje możliwość, iż stworzony opis sprzętu jest nieoptymalny dla danej architektury układu FPGA. Konieczne mogą okazać się znaczne zmiany projektowe po to, aby zmniejszyć konsumpcję zasobów~(elementów składowych bloków logicznych) i/lub móc osiągnąć wymaganą częstotliwość zegara. 

W związku z tym projektowanie funkcjonalności realizowanej dzięki układom FPGA jawi się jako proces o dużym stopniu komplikacji, gdzie nawet najmniejsza zmiana na którymkolwiek etapie może wpływać na jakość rezultatu końcowego~(ang. \textit{Quality of Results}, QoR). To czy dana implementacja jest satysfakcjonująca zależy zaś od postawionych lub narzuconych z zewnątrz wymagań. 

\section{Nowoczesne podejście do tworzenia funkcjonalnych układów elektronicznych}
W poprzednim podrozdziale wspomniano o językach opisu sprzętu, jako o narzędziu służącym do projektowania zachowania układów elektronicznych, które z natury rzeczy przetwarzają współbieżnie porcje danych. Języki te współistniały i były rozwijane równolegle z językami sekwencyjnymi służącymi do programowania procesorów, jednak z uwagi na powszechniejszy dostęp do układów typu CPU~(ang. \textit{Central Processing Unit}) znacznie bardziej rozpowszechnione zostały języki takie jak FORTRAN czy C, pozostawiając znajomość Verilog, VHDL i ich pochodne tylko dla wąskiego grona posługujących się nimi specjalistów.

W społeczności akademickiej oraz u producentów układów FPGA zrodziło się pytanie czy można udostępnić szerokiemu gronu odbiorców język, którego składnia nie odbiegałaby w znaczący sposób od znanych języków sekwencyjnych a pozwalający na przetwarzanie masowo równoległe~(ang. \textit{massively parallel}) podobnie, jak robią to HDL. Narzędzie dokonujące translacji kodu podobnego do C do jego ekwiwalentu w HDL, pozwoliłoby dotrzeć producentom układów FPGA do nowych odbiorców w znacznie prostszy sposób, a przez to zwiększyć swój zasięg i zyski. Mimo że próby stworzenia możliwie uniwersalnego narzędzia tego typu trwają już ponad 25 lat~\cite{C_VHDL}, dopiero niedawno stały się one wystarczająco użyteczne w realizacji postawionego im celu. 

\subsection{Synteza wysokiego poziomu}
Proces konwersji algorytmu~(rozumianego jako ciąg operacji koniecznych do wykonania w celu otrzymania żądanego wyniku na podstawie dostarczonych danych) opisanego w języku wysokiego poziomu do jego specyfikacji na poziomie RTL nazywany jest \textit{syntezą wysokiego poziomu}~(ang. \textit{high level synthesis}, HLS). Dokonuje ona analizy, kiedy żądane operacje mają być wykonane i wybiera odpowiednie bloki znajdujące się w układzie~(pamięć, moduły DSP, przerzutniki, LUT i inne), które te operacje będą wykonywać~(rysunek~\ref{ch2:img:HLS_schedule}). 
\addimage{chapters/ch2/img/HLS_schedule.png}{scale=0.30}{Przykład optymalizacji wykonania iloczynu skalarnego $\sum_{i = 0}^3 a_ib_i$ za pomocą HLS. Iloczyn skalarny 4-elementowych wektorów wymaga wykonania 4 operacji mnożenia i 3 dodawania przy czym część operacji może zostać wykonana współbieżnie. W pierwszym kroku HLS dokona alokacji 4 operatorów mnożenia~(I), następnie wykonane zostaną 2 sumy cząstkowe~(II) oraz suma końcowa~(III). Takie rozłożenie wykonania operacji w czasie zapewnia maksymalną wydajność, co wiąże się również z największym zużyciem zasobów znajdujących się w układzie. Narzędzia HLS umożliwiają również takie rozłożenie operacji w czasie, by kosztem zmniejszonej wydajności oszczędzić część zasobów logicznych}{Przykład optymalizacji wykonania iloczynu skalarnego za pomocą HLS}{ch2:img:HLS_schedule}
Wynikowy projekt zapisany w postaci RTL opisuje dla każdego kroku przepływ danych pomiędzy rejestrami, który realizuje funkcjonalność opisaną w prosty i zrozumiały sposób za pomocą języka wysokiego poziomu. Projektant/programista za cenę utraty kontroli nad formą RTL zdaje się w momencie konwersji na ciąg automatycznych procesów przewidzianych przez twórcę danego HLS\footnote{Szczegółowy opis tego, w jaki sposób dokonywana jest analiza algorytmu i jego reprezentacja za pomocą RTL można znaleźć w rozdziale 2~\cite{FPGA_SD}.}. W większości przypadków jednak samo wykrywanie operacji niezależnych, mogących być wykonanych jednocześnie w układzie jest niewystarczające, by wykorzystać pełen potencjał, jaki dają układy FPGA. Dlatego każdy HLS posiada w swojej składni pewien zestaw poleceń stanowiących wskazówkę dla procesu syntezy, w jaki sposób należy poddać konwersji daną partię kodu dla uzyskania optymalnych w danych warunkach rezultatów.

\section{Xilinx i Vivado HLS}


\subsection{Dobór układu w projekcie}

\section{Budowa oraz znaczenie poszczególnych elementów}

\section{Projektowanie funkcjonalności}
\subsection{Tradycyjne podejście}
\subsection{Synteza wysokiego poziomu}
co robi, co można a co nie, dyrektywy preprocesora,

\section{Sytnteza układu oraz implementacja w FPGA}
\subsection{PLUNIFY - INTIME}
???
\subsection{Vivado IP Integrator \& Processor Options}

