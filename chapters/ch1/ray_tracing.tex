\chapter{Śledzenie promieni}

\section{Współczesne techniki generowania trójwymiarowej grafiki komputerowej}
Dostęp do informacji (również wizualnej) jest motorem napędzającym rozwój współczesnego świata. Dokładność rozumiana jako wierność oddania zachowania się obiektu po wyprodukowaniu, ma znaczenie tak w przypadku projektu kryształowego wazonu jak i budynku filharmonii. Projektant dysponujący wizualizacją jest w stanie ocenić czy jest usatysfakcjonowany rezultatami swojej pracy czy wymagane są zmiany. Do wykonania takiej wizualizacji może posłużyć się jedną z dwóch ugruntowanych na przestrzeni ostatnich dziesięcioleci technik: rasteryzacją bądź też śledzeniem promieni (ang. \textit{ray tracing}). Ta pierwsza choć szybka pozwala uzyskiwać przybliżone rezultaty o skończonej dokładności~(stąd również jej popularność w branży elektronicznej rozrywki, gdzie dla komfortowej rozgrywki wymaga się generowania obrazu w tempie minimum 30~Hz). Druga zaś w prosty sposób umożliwia generowanie fizycznie poprawnych obrazów, jednak za cenę znacznego wydłużenia czasu obliczeń.

W następnych dwóch podrozdziałach zostaną przedstawione podstawy, które stoją za działaniem obu wyżej wspomnianych technik. 
\subsection{Rasteryzacja}
Proces rasteryzacji w ogólnym rozumieniu polega na transformacji grafiki wektorowej, czyli takiej, która opisywana jest poprzez położenia, kierunki i odległości, do płaskiego obrazu złożonego z określonej ilości pikseli w pionie i poziomie. W przypadku grafiki trójwymiarowej i posługiwania się biblioteką OpenGL~\cite{OPENGL46} czy wchodzącej w skład DirectX biblioteki Direct3D, wejście całego potoku przetwarzania geometrii stanowi zbiór obiektów (przeważnie trójkątów), z których każdy zbudowany jest z wierzchołków. Każdy wierzchołek $\vec{v_i}$ posiada zaś szereg atrybutów takich jak:
\begin{itemize}
\item macierz przekształceń 4x4 we współrzędnych obserwatora~(kamery) $\mathbf{MV_i}$ zawierającą informację o jego położeniu, skali, obrocie oraz ewentualnych deformacjach~(np. pochyleniu),
\item współrzędne tekstury,
\item normalna - o ile w przypadku pojedynczego punktu podawanie normalnej nie ma sensu, o tyle trzeba pamiętać, iż punkty te definiują odpowiednie płaszczyzny obiektów; przy takiej interpretacji można mówić o normalnej do płaszczyzny w danym jej wierzchołku, co wykorzystywane jest chociażby w procesie wizualnego bądź też rzeczywistego~(wykorzystując teselację\footnote{Technika dzielenia siatki obiektu na mniejsze, dzięki czemu obiekt wydaje się gładszy i dokładniejszy niż ten, który pierwotnie został przekazany do renderowania.}) wygładzania obiektów,
\item parametry dotyczące {\color{red}materiału\footnote{ODWOŁANIE???}}, z którego zbudowany jest opisywany obiekt,
\item i innych\footnote{W dobie programowalnych potoków przetwarzania programiści oraz artyści ograniczeni są właściwie tylko przez własną wyobraźnię. Dla przykładu animacja pomiędzy dwoma położeniami punktów będzie wymagać nie tylko aktualnej macierzy przekształceń, ale również drugiej odpowiadającej następnej klatce kluczowej animacji macierzy wraz z parametrem mówiącym o położeniu pomiędzy tymi dwoma stanami.}.
\end{itemize}
Atrybuty te są następnie wykorzystywane podczas kolejnych etapów tworzenia obrazu.

Ważnym globalnym parametrem jest również \textit{macierz projekcji} $\mathbf{P}$, która decyduje, jaka część świata zostanie odwzorowana na finalnym obrazie oraz w jaki sposób obiekty zostaną przez nią zdeformowane. Dokonuje ona transformacji w taki sposób, aby przekształcone za jej pomocą wierzchołki znajdujące się wewnątrz ściętego stożka widzenia\footnote{Rozpatrywany przypadek dotyczy rzutowania perspektywicznego. Nic jednak nie stoi na przeszkodzie by zdefiniować macierz $\mathbf{P}$ tak aby dokonywała innego przekształcenia np. izometrycznego.} miały współrzędne $[x,y] \in [-1, 1]^2$. Postać tej macierzy zależy od~\cite{PerspectiveMatrix}:
\begin{itemize}
\item odległości od obserwatora bliskiej \textit{n} oraz dalekiej \textit{f} płaszczyzny odcięcia,
\item kąta rozwarcia stożka widoczności $\mathrm{\alpha}$
\end{itemize}
\addimage{chapters/ch1/img/perspective_projection2.png}{scale=0.33}{Rzut z góry na stożek widoczności z pozycji obserwatora $O$, tylko obiekty wewnątrz stożka opisanego parametrami $n$, $f$, oraz $\alpha$ zostaną narysowane całkowicie, te na brzegach będą wygenerowane częściowo}{Rzut z góry na stożek widoczności z pozycji obserwatora $O$}{ch1:img:perspectiveMatrix}
i wygląda następująco:
\begin{equation}
P = 
\begin{bmatrix}
S & 0 & 0 & 0 \\ 
0 & S & 0 & 0 \\ 
0 & 0 & -\frac{f}{f-n} & -1\\ 
0 & 0 & -\frac{fn}{f-n} & 0
\end{bmatrix},\qquad \mathrm{gdzie\ }S = \frac{1}{\tan\frac{\alpha}{2}}.
\end{equation}
Wierzchołki przekształcone zgodnie z $\vec{v_{i}}^{'} = \mathbf{P}\cdot\mathbf{MV_i}\cdot \vec{v_i}$ i składające się na jeden prymityw geometryczny są na kolejnych etapach łączone ze sobą tworząc powierzchnię zajmującą określone pozycje pikseli na obrazie. 
\addimage{chapters/ch1/img/triangle.png}{scale=0.8}{Trójkąt po rasteryzacji; siatka kwadratów odpowiada niepodzielnej siatce pikseli obrazu, wierzchołki zostały oznaczone czerwonymi punktami i połączone ze sobą, z uwagi na fakt, iż piksele są elementami dyskretnymi, odpowiednie algorytmy muszą zdecydować czy dana płaszczyzna w odpowiednio dużym stopniu zajmuje miejsce odpowiadające danemu pikselowi, w efekcie powstają postrzępione granice obiektów, dla pikseli znajdujących się wewnątrz (niebieskie) należy dokonać interpolacji atrybutów}{Trójkąt po rasteryzacj}{ch1:img:triangleRaster}
Jako, że dokładne wartości atrybutów znane są tylko i wyłącznie dla wierzchołków to wewnątrz powierzchni atrybuty te muszą być interpolowane by móc na kolejnych etapach nadać pikselom pożądany kolor zgodny z przyjętym oświetleniem, materiałem i teksturą. Tak obliczone piksele, o ile znajdują się dalej w przestrzeni obserwatora, mogą zostać jeszcze nadpisane przez kolejne powierzchnie. Wykorzystywany jest do tego \textit{bufor głębokości}~(ang. \textit{z-buffer}), którego rozmiar w pikselach odpowiada generowanemu obrazowi, zaś same piksele przechowują informację o aktualnie najbliższym położeniu. Jeśli aktualnie przetwarzany obiekt~(płaszczyzna) dla danego piksela znajduje się bliżej niż wszystko, co do tej pory zostało wyrenderowane, wartość bufora głębokości zostaje nadpisana w tym punkcie przez nową wartość głębi, a piksel obrazu zostanie obliczony na nowo - tym razem biorąc pod uwagę własności tego nowego obiektu. W przeciwnym razie, gdy piksel odpowiadający nowemu obiektowi jest dalej w przestrzeni widoku zostaje on odrzucony. Dzięki czemu nie są przeprowadzane dalsze obliczenia dla elementów, których obserwator i tak nie jest w stanie dostrzec, oszczędzając przy tym czas procesora.

Powyższy opis procesu rasteryzacji, mimo iż znacznie uproszczony w stosunku do wykorzystywanych obecnie potoków przetwarzania~\cite{OPENGL_SUPERBIBLE}, zawiera sedno całej operacji i pozwala wyróżnić kilka jego najważniejszych cech.
\begin{enumerate}
\item Jest to proces, który idealnie nadaje się do zrównoleglenia obliczeń, dzięki temu, że każda płaszczyzna jest rysowana w sposób niezależny (synchronizacja danych jest wymagana jednak na etapie dostępu do bufora głębokości). Ma to swoje odzwierciedlenie w fakcie, że producenci procesorów graficznych od dawna produkują urządzenia zawierające setki a nawet tysiące równoległych jednostek przetwarzających~\cite{NV_Hardware}\cite{AMD_Hardware}.
\item Z uwagi na fakt, iż określenie pozycji w przestrzeni widoku wymaga znajomości pierwotnego położenia wierzchołka $\vec{v_i}$ to sam obiekt, którego część ten wierzchołek reprezentuje, musi zostać przybliżony przez sieć wierzchołków. Wynika z tego, iż chcąc uzyskać lepsze odwzorowanie krzywizny powierzchni należy dysponować gęstszą siatkę punktów.
\addimage{chapters/ch1/img/spheres.png}{scale=0.5}{Triangulacja obiektów na przykładzie sfery - im więcej podziałów powierzchni tym gładszy obiekt jednak czas renderowania dłuższy}{ch1:img:shpereTriangulation}{Triangulacja sfery}
\item Procesor przetwarzający aktualną powierzchnię nie wie nic o pozostałych obiektach w scenie. W szczególności nie wie nic o tym czy dana powierzchnia na drodze do źródła światła jest przesłaniana przez inne płaszczyzny czy też nie. W zwiąku z tym cienie generowane są kilkuetapowo. Najprostszą metodą jest wygenerowanie tzw. \textit{mapy cieni} (ang. \textit{shadow map}) z pozycji źródła światła. Jest to tekstura, która określa punkty, do których dociera oświetlenie. Wartości jej pikseli odpowiadają natomiast odległości najbliższych źródłu światła obiektów. W kolejnej fazie scena jest już renderowana normalnie z pozycji obserwatora i do określenia miejsc ocienionych używa się informacji z tekstury wygenerowanej wcześniej. Nie jest to jednak proces idealny. Z uwagi na fakt, że mapa cieni jest teksturą jej rozdzielczość w znaczący sposób wpływa na jakość obrazu - im większa tym granice między cieniem a powierzchnią oświetloną są mniej postrzępione. Dodatkowo stosuje się filtrację mapy cieni dla uzyskania gładszych przejść~\cite{GPU_GEMS3_PCF}. Podobne trudności sprawia również uzyskanie poprawnych (tzn. zgodnych z perspektywą i zasadami fizyki) odbić np. na powierzchniach lustrzanych czy wodzie. W prostych przypadkach, takich jak płaskie lustro, można sobie z tym poradzić poprzez narysowanie na płaszczyźnie lustrzanego odbicia sceny względem tej płaszczyzny. Jednak w większości wypadków powierzchnie odbijające posiadają skomplikowane kształty, przez co autorzy skłaniają się ku wykorzystaniu technik przybliżonych bądź też bazujących na śledzeniu promieni~\cite{GPU_GEMS3_MIRRORS}.
\end{enumerate}
W związku z przytoczonymi powyżej faktami można stwierdzić, iż rasteryzacja jest stosunkowo szybkim procesem (o ile obliczenia są prowadzone równolegle) opartym o informacje lokalne (dla danej powierzchni określonej przez zbiór jej wierzchołków), która musi posiłkować się różnymi (nieraz bardzo wyrafinowanymi) technikami przybliżonymi w celu stworzenia iluzji realizmu świata przedstawionego.

\subsection{Śledzenie promieni - zagadnienie widoczności}
Generowanie grafiki metodą śledzenia promieni jest oparte na fizycznych podstawach, które mówią o tym, w jaki sposób światło (fotony) propaguje się w przestrzeni. Cały proces polega na rekonstrukcji ścieżki, po której poruszają się fotony zanim trafią do obserwatora tworząc obraz~(Rys.~\ref{ch1:img:rayTracingView}) a pierwsze wzmianki o jego wykorzystaniu sięgają roku 1968~\cite{APPEL1968}.

Z pozycji obserwatora $\vec{O}$ w kierunku obserwacji $\vec{d_i}$ ($|\vec{d_i}| = 1$) wysyłane są promienie $\vec{r_i} = \vec{O} + t\vec{d_i}$, których celem jest znalezienie najbliższych obserwatorowi przeszkód~(obiektów) na drodze propagacji - są to tak zwane \textit{promienie pierwotne}~(ang. \textit{primary rays}). Aby znaleźć taki obiekt należy znać postać równania $f(x, y, z) = 0$ dla każdego obiektu znajdującego się w świecie i umieć je rozwiązać dla tego promienia. Rozwiązaniem takiego równania jest najmniejsza nieujemna wartość $t$ mówiąca o tym, jak daleko od obserwatora w zadanym kierunku $\vec{d_i}$ znajduje się obiekt (ujemne wartości $t$ odpowiadają sytuacji, gdy obiekt znajduje się za obserwatorem). Jeśli aktualna wartość $t$ jest mniejsza niż znane do tej pory $t_{min}$ następuje aktualizacja odległości $t_{min} = t$ oraz wielkości $p$ wskazującej na napotkany obiekt. W najprostszym przypadku (tzn. bez wykorzystania struktur akcelerujących typu siatki i drzewa~\cite{VINKLER_PHD}) znalezienie najbliższego obiektu $p$ dla promienia $\vec{r_i}$ wymaga sprawdzenia wszystkich obiektów w scenie.
\addimage{chapters/ch1/img/ray_tracing_view.png}{scale=0.15}{Wsteczna rekonstrukcja propagacji fotonów za pomocą śledzenia promieni~(rzut 2D z boku); Z pozycji obserwatora $\vec{O}$ wysyłane są promienie sprawdzające ścieżkę, po której poruszały się fotony zanim trafiły do obserwatora; Promień $\vec{r_1}$ w punkcie $A$ przecina sferę o środku $\vec{P}$, następnie zostaje odbity $\vec{r_r}$ względem normalnej $\vec{n}$ w punkcie $A$ oraz załamany $\vec{r_t}$ (promienie wtórne); relacja pomiędzy kątami $\angle AOG$ i $\angle LAB$ wpływa na kolor obiektu w danym punkcie zgodnie z przyjętym modelem materiału oraz parametrami opisującymi źródło światła $L$; W przypadku promienia $\vec{r_2}$ przecinającego płaszczyznę wyznaczoną przez punkty $D$ oraz $E$ w punkcie $F$, promień cienia $\overrightarrow{FL}$ napotyka na swojej drodze przeszkodę w postaci sfery, w skutek czego punkt ten będzie znajdował się w cieniu (tak jak wszystkie punkty położone na odcinku $|DE|$)}{Wsteczna rekonstrukcja propagacji fotonów za pomocą śledzenia promieni}{ch1:img:rayTracingView}
Znalezienie punktu $X$, w którym nastąpiło przecięcie promienia $r_i$ z najbliższym mu obiektem pozwala na obliczenie m. in. normalnej $\vec{n}$ w danym punkcie. Jest to kluczowy parametr, wykorzystywany w obliczeniu tak koloru wynikającego z położenia względem źródła światła jak i do stworzenia promieni, których początek $\vec{O'}$ znajduje się w $X$ zaś kierunek odpowiada promieniu (tzw. \textit{promienie wtórne} ang. \textit{secondary rays}):
\begin{itemize}
\item odbitemu,
\item załamanemu,
\item tzw. \textit{promieniu cienia} (ang. \textit{shadow ray}) sprawdzającemu czy na drodze między badanym punktem $X$ a źródłem światła nie znajduje się inny obiekt go przysłaniający (rzucający cień na $X$).
\end{itemize}
Dla tych promieni również sprawdzane są testy przecięcia ze wszystkimi obiektami, a efekty tych obliczeń wpływają na wynikowy kolor piksela, który powiązany jest z konkretnym promieniem pierwotnym $\vec{r_i}$. W szczególności promienie wtórne mogą rozdzielać się na kolejne wtórne promienie (mówi się o tzw. głębokości śledzenia promieni). Im większa głębokość śledzenia promieni tym większy stopień realizmu jak również złożoność obliczeń.

Przedstawiona powyżej technika choć bardzo prosta w swoich założeniach pozwala generować złożone wizualizacje o wysokim poziomie realizmu. Co bardzo istotne w kontekście porównania do rasteryzacji, generowanie cieni czy odbić nie wymaga specjalnego traktowania i tworzenia dodatkowych algorytmów. Wymagana jest jedynie znajomość położenia obserwatora $\vec{O}$ oraz listy obiektów, których przecięcie z promieniem jesteśmy w stanie obliczyć w sposób analityczny. Wynika stąd również fakt, że nie jest wymagane by obiekty sceny były opisywane poprzez listę ich wierzchołków. Dla przykładu sferę można wyrenderować jako zbiór odpowiedniej ilości trójkątów jak i rozwiązując analityczne równanie $(\vec{r_i} - \vec{o})^2 = R^2$, gdzie $\vec{o}$ i $R$ to odpowiednio położenie środka oraz promień sfery. W pierwszym przypadku algorytm musi sprawdzić przecięcia ze wszystkimi trójkątami, podczas gdy w drugim wystarczy tylko rozwiązać równanie kwadratowe, które ponadto poda niemal dokładne rozwiązanie\footnote{Należy mieć na uwadze, iż komputerowe obliczenia zmiennoprzecinkowe charakteryzują się skończoną dokładnością.}. 

W porównaniu do rasteryzacji, śledzenie promieni charakteryzuje się też innym pochodzeniem równoległości obliczeń. W tym przypadku żaden promień (pierwotny, wtórne, cieni), który uczestniczy w tworzeniu koloru danego piksela nie wpływa na kolor innych pikseli. Stąd idealny system śledzący promienie byłby w stanie przetwarzać wszystkie piksele jednocześnie.

Niestety największą wadą a przez to czynnikiem niepozwalającym na adaptację śledzenia promieni w znacznej części przypadków jest powolność. Każdy stworzony promień w scenie musi na podstawie dostarczonej listy obiektów sam określić, który (jeśli którykolwiek) z nich znajduje się najbliżej na jego drodze. Jest to zupełne przeciwieństwo sytuacji, która ma miejsce podczas rasteryzacji, gdzie pozycja na mapie pikseli każdego obiektu geometrycznego z listy jest określana tylko raz dla danego obrazu. Stąd koniecznym jest stosowanie struktur akcelerujących{\color{red}ODNOSNIKI!!!}, pozwalających na szybkie stwierdzenie czy i jaki podzbiór wszystkich obiektów znajduje się w danym kierunku propagacji promienia $\vec{r_i}$.
\section{Fizyczne podstawy generowania realistycznych obrazów}
\subsection{Bieg promieni świetlnych w przestrzeni}
\subsection{Fotometria}
\subsubsection{Modelowanie materiałów}
Lambert, Phong, Oren-Nayar, Torrance-Sparrow
\subsection{Dystrybucja energii na granicy dwóch ośrodków - równania Fresnela}

\section{Metoda Monte Carlo}
\subsection{Przykłady zastosowań}
symulacja rozmycia obrazu, źródła światła skończonych rozmiarów, lśniące powierzchnie